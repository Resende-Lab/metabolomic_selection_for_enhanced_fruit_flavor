---
title: "5.a.1.calculate_final_weights"
author: "Colantonio and Ferrao et al., 2022"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    toc: yes
    toc_depth: '3'
header-includes: \usepackage{float}    
editor_options: 
  chunk_output_type: console
---

```{r,echo=F,include=FALSE}

library(caret)

library(BGLR)
library(elasticnet)
library(glmnet)
library(nnet)
library(neuralnet)
library(pls)
library(randomForest)
library(kernlab)
library(brnn)
library(gbm)
library(xgboost)
library(spls)

library(foreach)
library(doParallel)

library("qualityTools")
library(dplyr)
library(tidyr)

```

 


```{r}

#bb = read.csv("./data/bb_imputed_scaled.csv", header = T, check.names = F, na.strings = ".")
tom = read.csv("./data/tom_imputed_scaled.csv", header = T, check.names = F, na.strings = ".")

traits <- colnames(tom)[2:8]
mets <- colnames(tom)[9:76]
  
bayes <- c("BayesA", "BayesB", "BayesC", "BRR", "BL")
mods <- c(bayes,
        "RandomForest",
        "SupportVectorMachineLinear",
        "ElasticNet",
        "NeuralNet",
        "KernelPartialLeastSquares",
        "LinearRegression",
        "RKHS",
        "GradientBoostMachine",
        "ReleventVectorMachine",
        "SVMRadial",
        "SVMRadialSigma",
        "BayesianNeuralNet",
        "Xgboost")


run <- format(Sys.time(), "%y%m%d_%I%M%p")

dir.create(paste("./data/Results_", run, sep = ""))


```


```{r}
#RegisterParallel
no_cores <- detectCores() - 4
cl <- makeCluster(no_cores)
registerDoParallel(cl)
```


```{r}
####################
# Begin Experiment #
####################

  NMOD <- 18
  NPHEN <- 7
  NMET <- 68
  NFOLD <- 10
  
  # Gather Initial Seeds
  set.seed(0)
  seeds <- sample(1:1e4, 10)

    

  
for(experiment in 2:10){

  tom = read.csv("./data/tom_imputed_scaled.csv", header = T, check.names = F, na.strings = ".")

  seed <- seeds[experiment]
  set.seed(seed)

# Create Result Matrices
  acc <- matrix(NA, nrow = NMOD, ncol = 1)
  mse <- matrix(NA, nrow = NMOD, ncol = 1)
  wts <- matrix(NA, nrow = NMOD, ncol = NMET)

   
  start <- proc.time()

  tomresults <- foreach(trait = c(1:7), 
                        .combine = 'rbind', 
                        .multicombine = TRUE, 
                        .packages = c("BGLR", 
                                      "caret", 
                                      "randomForest", 
                                      "pls", 
                                      "nnet", 
                                      "glmnet", 
                                      "neuralnet", 
                                      "gbm")) %do% {   
               
        set.seed(paste(seed, trait, experiment, sep = ""))

        trnX <- as.matrix(tom[, (2+NPHEN):(1+NPHEN+NMET)])
        trnY <- as.matrix(tom[, trait + 1])

        #BAYES
        for(method in 1:1){
          ETA <- list(X = list(X = trnX, model = bayes[method]))
          fit <- BGLR(y = trnY, ETA = ETA, nIter = 30000, burnIn = 10000, thin = 100, verbose = F)
          wts[method,] <- fit$ETA$X$b 
        }      
        
        #CARET MODELS
        carX <- as.matrix(cbind(Y = tom[, trait + 1], tom[, (2+NPHEN):(1+NPHEN+NMET)]))

  			#StochasticGradientBoostMachine
        grid <- expand.grid(.n.trees = c(500, 750, 1000), .interaction.depth = c(5,7,10), .shrinkage = c(0.01,0.02,0.05), .n.minobsinnode = c(3,5))
        trnCtrl <- trainControl(method = "cv", number = NFOLD)
        gbm <- train(Y ~ ., data = carX, method = "gbm", trControl = trnCtrl, tuneGrid = grid)
  			wts[13,] <- as.matrix(varImp(object = gbm)$importance)
  		
        return(wts[c(1,13), ])
        cat(trait)
      }
  
  results <- data.frame(tomresults)  
  colnames(results) <- colnames(tom)[(2+NPHEN):(1+NPHEN+NMET)]
  
  results$model <- rep(c("BayesA", "GradientBoostMachine"), 7)
  results$experiment <- experiment
  results$trait <- c(rbind(traits, traits))
  results <- results[,c(69, 70, 71, 1:68)]

  if(experiment == 1){
    resultsFull <- results  
  } else {
    resultsFull <- rbind(resultsFull, results)
  }
  
  rm(list = ls()[!ls() %in% c("resultsFull", "traits", "mods", "mets", "bayes", "run", "NMOD", "NMET", "NPHEN", "NFOLD", "seeds", "cl", "mainlog")])
}
  
#  resultsFull$trait <- rep(c(rbind(traits, traits)), 10)
#  resultsFull <- resultsFull[, c(1, 2, 71, 3:70)]
  
  resFull <- melt(resultsFull, id.vars = c("model", "experiment", "trait"))
  
  colnames(resFull)[c(4:5)] <- c("metabolite", "weight")
  
#  resplot <- aggregate(resFull$weight, by = list(resFull$model, resFull$metabolite, resFull$trait), FUN = mean)
#  resplot <- dcast(resplot, Group.2 + Group.3 ~  Group.1)
  
  colnames(resplot)[1:2] <- c("metabolite", "trait")
  
  ggplot(resplot, 
         aes(x = GradientBoostMachine, y = BayesA, color = metabolite)) + 
    geom_point() + 
    facet_wrap(~trait, ncol = 4) + 
#    geom_text_repel(label = resplot$metabolite, size = 3) +
    theme(legend.position = c(0.85, 0.2))
  
  resultsFull <- resultsFull[, c(1,2,4:71,3)]
  resplot <- dcast(resFull, metabolite + trait + experiment ~  model)

    
```





##################
##################
##################
## BLUEBERRY ##
###############
###############


# Create Directories

```{r}

bb = read.csv("./data/bb_imputed_scaled.csv", header = T, check.names = F, na.strings = ".")
#tom = read.csv("./data/tom_imputed_scaled.csv", header = T, check.names = F, na.strings = ".")

traits <- colnames(bb)[2:5]
mets <- colnames(bb)[6:dim(bb)[2]]
bayes <- c("BayesA", "BayesB", "BayesC", "BRR", "BL")
mods <- c(bayes,
        "RandomForest",
        "SupportVectorMachineLinear",
        "ElasticNet",
        "NeuralNet",
        "KernelPartialLeastSquares",
        "LinearRegression",
        "RKHS",
        "GradientBoostMachine",
        "ReleventVectorMachine",
        "SVMRadial",
        "SVMRadialSigma",
        "BayesianNeuralNet",
        "Xgboost")


NPHEN <- length(traits)
NMET <- length(mets)
NMOD <- 18
NFOLD <- 10


run <- format(Sys.time(), "%y%m%d_%I%M%p")

dir.create(paste("./data/bb_Results_", run, sep = ""))

```



```{r}
####################
# Begin Experiment #
####################

  NMOD <- 18
  NPHEN <- length(traits)
  NMET <- length(mets)
  NFOLD <- 10
  
  # Gather Initial Seeds
  set.seed(0)
  seeds <- sample(1:1e4, 10)
    

for(experiment in 1:10){

 
  bb = read.csv("./data/bb_imputed_scaled.csv", header = T, check.names = F, na.strings = ".")

  seed <- seeds[experiment]
  set.seed(seed)

  
  wts <- matrix(NA, nrow = NMOD, ncol = NMET)

  start <- proc.time()

  bbresults <- foreach(trait = c(1:NPHEN), 
                        .combine = 'rbind', 
                        .packages = c("BGLR", 
                                      "caret", 
                                      "randomForest", 
                                      "pls", 
                                      "nnet", 
                                      "glmnet", 
                                      "neuralnet", 
                                      "gbm")) %do% {   

        set.seed(paste(seed, trait, experiment, sep = ""))

        trnX <- as.matrix(bb[, (2+NPHEN):(1+NPHEN+NMET)])
        trnY <- as.matrix(bb[, trait + 1])

        #BAYES
        for(method in 1:1){
          ETA <- list(X = list(X = trnX, model = bayes[method]))
          fit <- BGLR(y = trnY, ETA = ETA, nIter = 30000, burnIn = 10000, thin = 100, verbose = F)
          wts[method,] <- fit$ETA$X$b 
        }      
      
        #CARET MODELS
        carX <- as.matrix(cbind(Y = bb[, trait + 1], bb[, (2+NPHEN):(1+NPHEN+NMET)]))

  			#StochasticGradientBoostMachine
        grid <- expand.grid(.n.trees = c(500, 750, 1000), .interaction.depth = c(5,7,10), .shrinkage = c(0.01,0.02,0.05), .n.minobsinnode = c(3,5))
        trnCtrl <- trainControl(method = "cv", number = NFOLD)
        gbm <- train(Y ~ ., data = carX, method = "gbm", trControl = trnCtrl, tuneGrid = grid)
  			wts[13,] <- as.matrix(varImp(object = gbm)$importance)
  		
        return(wts[c(1,13), ])
        cat(trait)
      }
  
  results <- data.frame(bbresults)  
  colnames(results) <- colnames(bb)[(2+NPHEN):(1+NPHEN+NMET)]
  
  results$model <- rep(c("BayesA", "GradientBoostMachine"), 4)
  results$experiment <- experiment
  results$trait <- c(rbind(traits, traits))
  results <- results[,c(56:58, 1:55)]

  if(experiment == 1){
    resultsFull <- results  
  } else {
    resultsFull <- rbind(resultsFull, results)
  }
  
  rm(list = ls()[!ls() %in% c("resultsFull", "traits", "mods", "mets", "bayes", "run", "NMOD", "NMET", "NPHEN", "NFOLD", "seeds", "cl", "mainlog")])
}
  

  resFull <- melt(resultsFull, id.vars = c("model", "experiment", "trait"))
  
  colnames(resFull)[c(4:5)] <- c("metabolite", "weight")
  
#  resplot <- aggregate(resFull$weight, by = list(resFull$model, resFull$metabolite, resFull$trait), FUN = mean)
#  resplot <- dcast(resplot, Group.2 + Group.3 ~  Group.1)
  resplot <- dcast(resFull, metabolite + trait + experiment ~  model)
  
  colnames(resplot)[1:2] <- c("metabolite", "trait")
  
  ggplot(resplot, 
         aes(x = GradientBoostMachine, y = BayesA, color = metabolite)) + 
    geom_point() + 
    facet_wrap(~trait, ncol = 2) + 
    geom_text_repel(label = resplot$metabolite, size = 3) +
    theme(legend.position = "top")
  

   
  
```
















#### Weights for Inference

```{r,fig.height=10}

weights <- read.csv("tomatoFinalWeights.csv")

weights <- aggregate(weights[,4:5], by = list(weights$metabolite, weights$trait), FUN = mean)
colnames(weights)[1:2] <- c("metabolite", "trait")

nodecols <- read.csv("./tomNetKey.csv")
nodecols <- nodecols[order(match(nodecols[,1], weights$metabolite)),]
nodecols <- nodecols[-c(69,70),]

labs <- list()

#bitter
labs[[1]] <- c("citric", "glutamic acid", "malic acid", "benzyl cyanide", "salicylaldehyde", "glucose", "fructose", "cis-4-decenal", "guaiacol", "b-ionone")

#intensity
labs[[2]] <- c("citric", "trans-2-pentenal", "Soluble solids", "glucose", "fructose", "trans-2-heptenal", "trans-2-hexenal", "glutamic acid", "benzaldehyde", "guaiacol")

#liking
labs[[3]] <- c("glucose", "fructose", "Soluble solids", "malic acid", "4-carene", "benzyl alcohol", "trans-3-hexen-1-ol", "3-pentenone", "citric","trans-2-pentenal")

#salty
labs[[4]] <- c("citric", "trans-2-pentenal", "Soluble solids", "isovaleric acid", "3-pentenone", "glutamic acid", "3-pentanone")

#sour
labs[[5]] <- c("citric", "malic acid", "2-butylacetate", "propyl acetate", "cis-4-decenal", "fructose", "glutamic acid", "Soluble solids","benzothiazole")

#sweetness
labs[[6]] <- c("glucose", "fructose", "trans-2-pentenal", "4-carene", "trans-2-hexenal", "prenyl acetate", "Soluble solids", "citric", "2-phenyl ethanol")

#umami
labs[[7]] <- c("trans-2-pentenal", "glutamic acid", "1-nitro-2-phenylethane", "benzyl cyanide", "2-phenyl ethanol", "citric", "geranial", "2-methylbutyraldehyde", "trans-3-hexen-1-ol", "trans-2-hexenal")


firstup <- function(x) {
  substr(x, 1, 1) <- toupper(substr(x, 1, 1))
  x
}

weights$trait <- firstup(as.character(weights$trait))
weights$label <- ""

for(i in 1:7){
    weights$label[weights$metabolite %in% labs[[i]] & weights$trait == unique(weights$trait)[i]] <- as.character(weights$metabolite[weights$metabolite %in% labs[[i]] & weights$trait == unique(weights$trait)[i]])
}


#For true classifications
#weights$color <- rep(nodecols$type, 7)

#For classification by network assigned modules
weights$color <- rep(nodecols$type2, 7)

weights$size <- ((abs(weights$BayesA) * weights$GradientBoostMachine * 0.1) + 1)
weights$size2 <- ((abs(weights$BayesA) * weights$GradientBoostMachine * 0.2) + 4.5)

p <- ggplot(weights, aes(x = GradientBoostMachine, y = BayesA, color = color, label = (label)))+
      geom_point(size = weights$size)+
      theme_bw()+  

  geom_text_repel(size = weights$size2, 
                  min.segment.length = 0,
                  point.padding = 0.25, 
                  segment.color = "grey", 
                  show.legend = F) +
  
  facet_wrap(~trait, ncol = 4) +
  
  theme(strip.background =element_rect(fill=brewer.pal(n = 4, name = "Set3")[4])) +
  scale_color_manual(name = "Groups",values = brewer.pal(n = 12, name = "Set3")[c(8,1,11,6,3:4,7,9:10,5)]) +

  theme(strip.text = element_text(size = 24),
        axis.title = element_text(size = 24),
        axis.text = element_text(size = 18),
        legend.text = element_text(size = 18), 
        legend.spacing.y = unit(0.5, 'cm'),
        legend.key = element_rect(color = NA, fill = NA),
        legend.key.size = unit(1, "cm"),
        legend.title = element_text(size = 24),
        legend.position = c(0.85, 0.2))

ggsave(file="TomatoWeights.svg", plot=p, width=22, height=10)

```



#### Weights for Inference

```{r,fig.height=10}

weights <- read.csv("blueberryFinalWeights.csv")

weights <- aggregate(weights[,4:5], by = list(weights$metabolite, weights$trait), FUN = mean)
colnames(weights)[1:2] <- c("metabolite", "trait")

nodecols <- read.csv("./bbNetKey.csv")
nodecols <- nodecols[order(match(nodecols[,1], weights$metabolite)),]
nodecols <- nodecols[-c(69,70),]

labs <- list()

#intensity
labs[[1]] <- c("firmness", "2-Undecanone", "Soluble solids", "Ethyl propionate", "Methyl isovalerate", "Caryophyllene", "Valeraldehyde", "Methyl salicylate")

#liking
labs[[2]] <- c("2-Undecanone", "fructose", "Soluble solids", "Eucalyptol", "Hexanal", "Ethyl propionate", "firmness", "Valeraldehyde", "Phenylacetaldehyde","Methyl isovalerate", "Methyl salicylate", "TA", "Linalool")

#sour
labs[[3]] <- c("TA", "Eucalyptol", "2-Hexenyl-acetate", "sucrose", "Linalool", "Neral", "3-Hexenyl-acetate", "2-Hexen-1-al", "citric")

#sweetness
labs[[4]] <- c("TA", "fructose", "Valeraldehyde", "Eucalyptol", "2-Nonanone", "firmness", "2-Undecanone", "Phenylacetaldehyde", "Ethyl propionate", "Soluble solids")


firstup <- function(x) {
  substr(x, 1, 1) <- toupper(substr(x, 1, 1))
  x
}

weights$trait <- firstup(as.character(weights$trait))
weights$label <- ""

for(i in 1:4){
    weights$label[weights$metabolite %in% labs[[i]] & weights$trait == unique(weights$trait)[i]] <- as.character(weights$metabolite[weights$metabolite %in% labs[[i]] & weights$trait == unique(weights$trait)[i]])
}


#For true classifications
#weights$color <- rep(nodecols$type, 7)

#For classification by network assigned modules
weights$color <- rep(nodecols$type2, 4)

weights$size <- ((abs(weights$BayesA) * weights$GradientBoostMachine * 0.1) + 1)
weights$size2 <- ((abs(weights$BayesA) * weights$GradientBoostMachine * 0.15) + 8)


p <- ggplot(weights, aes(x = GradientBoostMachine, y = BayesA, color = color, label = (label)))+
      geom_point(size = weights$size)+
      theme_bw()+  

  geom_text_repel(size = weights$size2, 
                  min.segment.length = 0,
                  point.padding = 0.25, 
                  segment.color = "grey", 
                  show.legend = F) +
  
  facet_wrap(~trait, ncol = 4) +
  
  theme(strip.background =element_rect(fill=brewer.pal(n = 8, name = "Set3")[5])) +
  scale_color_manual(name = "Groups",values = brewer.pal(n = 12, name = "Set3")[c(8,1,11,6,3:4,7,9:10,5)]) +

  theme(strip.text = element_text(size = 24),
        axis.title = element_text(size = 24),
        axis.text = element_text(size = 18),
        legend.text = element_text(size = 18), 
        legend.spacing.y = unit(0.5, 'cm'),
        legend.key = element_rect(color = NA, fill = NA),
        legend.key.size = unit(1, "cm"),
        legend.title = element_text(size = 24),
        legend.position = "right")

ggsave(file="bbWeights.svg", plot=p, width=26, height=8)

```

